{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7750232,"sourceType":"datasetVersion","datasetId":4530888},{"sourceId":7756943,"sourceType":"datasetVersion","datasetId":4535932}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install bitsandbytes einops wandb -Uqqq\n!pip install -Uqqq datasets===2.16.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-26T15:43:15.901859Z","iopub.execute_input":"2024-03-26T15:43:15.902859Z","iopub.status.idle":"2024-03-26T15:44:29.435406Z","shell.execute_reply.started":"2024-03-26T15:43:15.902824Z","shell.execute_reply":"2024-03-26T15:44:29.434376Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ns3fs 2024.3.0 requires fsspec==2024.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom huggingface_hub import notebook_login\nimport pandas as pd\nimport torch\nfrom trl import SFTTrainer\n\nSEED = 999\nBATCH_SIZE = 32\ntorch.manual_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:44:29.437641Z","iopub.execute_input":"2024-03-26T15:44:29.438057Z","iopub.status.idle":"2024-03-26T15:44:48.548307Z","shell.execute_reply.started":"2024-03-26T15:44:29.438023Z","shell.execute_reply":"2024-03-26T15:44:48.547357Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-26 15:44:37.624880: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-26 15:44:37.625016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-26 15:44:37.756087: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7e2bb94c8950>"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/lyrics-dataset-rock-pop-rap-metal-indie/train_dataset.tsv\", sep=\"\\t\")","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:56:08.605055Z","iopub.execute_input":"2024-03-26T15:56:08.605781Z","iopub.status.idle":"2024-03-26T15:56:09.449947Z","shell.execute_reply.started":"2024-03-26T15:56:08.605751Z","shell.execute_reply":"2024-03-26T15:56:09.449108Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## SELECT ONLY THREE GENRES TO SPEED UP TRAINING\n\ndf_rock = df[df['genre'] == \"rock\"]\ndf_rap = df[df['genre'] == \"rap\"]\ndf_pop = df[df['genre'] == \"pop\"]\ndf = pd.concat([df_rock, df_rap, df_pop])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:56:09.661408Z","iopub.execute_input":"2024-03-26T15:56:09.661716Z","iopub.status.idle":"2024-03-26T15:56:09.692119Z","shell.execute_reply.started":"2024-03-26T15:56:09.661692Z","shell.execute_reply":"2024-03-26T15:56:09.691027Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                 lyrics genre  \\\n6400  [Segue – Intro:\\nGerry O'Driscoll\\n]\\n…and I a...  rock   \n6401  [Verse 1]\\n'Cause you're a sky, 'cause you're ...  rock   \n6402  [Piano Intro]\\n[Verse 1]\\nOnly love can make i...  rock   \n6403  [Verse 1]\\nDrowning in their dissertations\\nRa...  rock   \n6404  [Intro]\\nOoooooooooh!!!\\n[Verse 1]\\nAll around...  rock   \n...                                                 ...   ...   \n4795  What are you fucking crazy?\\nDamn ...\\nIt's ge...   pop   \n4796  [Verse 1: PARTYNEXTDOOR]\\nThat thing go raw\\nT...   pop   \n4797  [Verse 1]\\nComparisons are easily done\\nOnce y...   pop   \n4798  [Verse 1]\\nHere I am waiting\\nI'll have to lea...   pop   \n4799  [Chorus]\\nYou can pretend you don't miss me (M...   pop   \n\n                     artist                                               text  \n6400             Pink Floyd  Below is an instruction that describes a task....  \n6401               Coldplay  Below is an instruction that describes a task....  \n6402                The Who  Below is an instruction that describes a task....  \n6403              Pearl Jam  Below is an instruction that describes a task....  \n6404  Red Hot Chili Peppers  Below is an instruction that describes a task....  \n...                     ...                                                ...  \n4795             Bruno Mars  Below is an instruction that describes a task....  \n4796             Bruno Mars  Below is an instruction that describes a task....  \n4797             Katy Perry  Below is an instruction that describes a task....  \n4798               Maroon 5  Below is an instruction that describes a task....  \n4799          Billie Eilish  Below is an instruction that describes a task....  \n\n[4800 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lyrics</th>\n      <th>genre</th>\n      <th>artist</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6400</th>\n      <td>[Segue – Intro:\\nGerry O'Driscoll\\n]\\n…and I a...</td>\n      <td>rock</td>\n      <td>Pink Floyd</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>6401</th>\n      <td>[Verse 1]\\n'Cause you're a sky, 'cause you're ...</td>\n      <td>rock</td>\n      <td>Coldplay</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>6402</th>\n      <td>[Piano Intro]\\n[Verse 1]\\nOnly love can make i...</td>\n      <td>rock</td>\n      <td>The Who</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>6403</th>\n      <td>[Verse 1]\\nDrowning in their dissertations\\nRa...</td>\n      <td>rock</td>\n      <td>Pearl Jam</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>6404</th>\n      <td>[Intro]\\nOoooooooooh!!!\\n[Verse 1]\\nAll around...</td>\n      <td>rock</td>\n      <td>Red Hot Chili Peppers</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4795</th>\n      <td>What are you fucking crazy?\\nDamn ...\\nIt's ge...</td>\n      <td>pop</td>\n      <td>Bruno Mars</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>4796</th>\n      <td>[Verse 1: PARTYNEXTDOOR]\\nThat thing go raw\\nT...</td>\n      <td>pop</td>\n      <td>Bruno Mars</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>4797</th>\n      <td>[Verse 1]\\nComparisons are easily done\\nOnce y...</td>\n      <td>pop</td>\n      <td>Katy Perry</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>4798</th>\n      <td>[Verse 1]\\nHere I am waiting\\nI'll have to lea...</td>\n      <td>pop</td>\n      <td>Maroon 5</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n    <tr>\n      <th>4799</th>\n      <td>[Chorus]\\nYou can pretend you don't miss me (M...</td>\n      <td>pop</td>\n      <td>Billie Eilish</td>\n      <td>Below is an instruction that describes a task....</td>\n    </tr>\n  </tbody>\n</table>\n<p>4800 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def create_song_text_tiny_llama(row):\n    genre = row['genre']\n    lyrics = row['lyrics']\n    return f\"<s>[INST] You are an AI trained to generate lyrics for songs of those genres: Rock, Metal, Pop, Indie. Your task is to ensure that the generated lyrics reflect the true essence of the genre given in input. A Rock or Metal song will have strong and direct lyrics while a Pop or Indie song is generally softer and happier. Your output should be as close as possible to the genre given in input. Generate lyrics for a {genre} song. [/INST] {lyrics}\"\n\n\ndef create_song_text_phi(row):\n    genre = row['genre']\n    lyrics = row['lyrics']\n    return f\"### Instruction: Generate lyrics for a {genre} song. ### Assistant: {lyrics}\"\n    \n\ndf['text'] = df.apply(create_song_text_tiny_llama, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:56:25.241331Z","iopub.execute_input":"2024-03-26T15:56:25.242251Z","iopub.status.idle":"2024-03-26T15:56:25.325551Z","shell.execute_reply.started":"2024-03-26T15:56:25.242218Z","shell.execute_reply":"2024-03-26T15:56:25.324859Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(len(df))\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:56:27.151139Z","iopub.execute_input":"2024-03-26T15:56:27.151823Z","iopub.status.idle":"2024-03-26T15:56:27.161129Z","shell.execute_reply.started":"2024-03-26T15:56:27.151791Z","shell.execute_reply":"2024-03-26T15:56:27.160052Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"4800\n                                                 lyrics genre  \\\n6400  [Segue – Intro:\\nGerry O'Driscoll\\n]\\n…and I a...  rock   \n6401  [Verse 1]\\n'Cause you're a sky, 'cause you're ...  rock   \n6402  [Piano Intro]\\n[Verse 1]\\nOnly love can make i...  rock   \n6403  [Verse 1]\\nDrowning in their dissertations\\nRa...  rock   \n6404  [Intro]\\nOoooooooooh!!!\\n[Verse 1]\\nAll around...  rock   \n...                                                 ...   ...   \n4795  What are you fucking crazy?\\nDamn ...\\nIt's ge...   pop   \n4796  [Verse 1: PARTYNEXTDOOR]\\nThat thing go raw\\nT...   pop   \n4797  [Verse 1]\\nComparisons are easily done\\nOnce y...   pop   \n4798  [Verse 1]\\nHere I am waiting\\nI'll have to lea...   pop   \n4799  [Chorus]\\nYou can pretend you don't miss me (M...   pop   \n\n                     artist                                               text  \n6400             Pink Floyd  <s>[INST] You are an AI trained to generate ly...  \n6401               Coldplay  <s>[INST] You are an AI trained to generate ly...  \n6402                The Who  <s>[INST] You are an AI trained to generate ly...  \n6403              Pearl Jam  <s>[INST] You are an AI trained to generate ly...  \n6404  Red Hot Chili Peppers  <s>[INST] You are an AI trained to generate ly...  \n...                     ...                                                ...  \n4795             Bruno Mars  <s>[INST] You are an AI trained to generate ly...  \n4796             Bruno Mars  <s>[INST] You are an AI trained to generate ly...  \n4797             Katy Perry  <s>[INST] You are an AI trained to generate ly...  \n4798               Maroon 5  <s>[INST] You are an AI trained to generate ly...  \n4799          Billie Eilish  <s>[INST] You are an AI trained to generate ly...  \n\n[4800 rows x 4 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\ndataset = Dataset.from_pandas(df)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:56:30.145015Z","iopub.execute_input":"2024-03-26T15:56:30.145368Z","iopub.status.idle":"2024-03-26T15:56:30.393895Z","shell.execute_reply.started":"2024-03-26T15:56:30.145341Z","shell.execute_reply":"2024-03-26T15:56:30.392980Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['lyrics', 'genre', 'artist', 'text', '__index_level_0__'],\n    num_rows: 4800\n})"},"metadata":{}}]},{"cell_type":"code","source":"#model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v0.1\"\nmodel_name = \"microsoft/phi-2\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.float16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    device_map={\"\": 0}\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel.config.use_cache = False # re-enable for inference\nmodel.config.pretraining_tp = 1\n# Load Model tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side = \"right\")\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.add_eos_token = True","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:44:48.549403Z","iopub.execute_input":"2024-03-26T15:44:48.549918Z","iopub.status.idle":"2024-03-26T15:45:14.347729Z","shell.execute_reply.started":"2024-03-26T15:44:48.549892Z","shell.execute_reply":"2024-03-26T15:45:14.346671Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f2e34b22b4241ab9884ebc192971788"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6fbcb765003418bbdea19002f9a145c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/63.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f74ee48426447b69e58e16a7e9bd1df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"606f375d3c2d4796bc03f614585790e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af57537694140d3b4a787f72f2b87ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cafb358b72f433aa0bc686590b25b94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aee9c1c07e3479e8172bdd643087ea6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358285cc971a4f02a0478257564e8076"}},"metadata":{}}]},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"YOUR_WANDB_KEY\")\nrun = wandb.init(project='Fine tuning LLM for lyrics generation', job_type=\"training\", anonymous=\"allow\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:01:53.210002Z","iopub.execute_input":"2024-03-24T13:01:53.210651Z","iopub.status.idle":"2024-03-24T13:02:27.510435Z","shell.execute_reply.started":"2024-03-24T13:01:53.210604Z","shell.execute_reply":"2024-03-24T13:02:27.509399Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mteglia-simone\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240324_130155-guq02co0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/teglia-simone/Fine%20tuning%20LLM%20for%20lyrics%20generation/runs/guq02co0' target=\"_blank\">wild-rain-14</a></strong> to <a href='https://wandb.ai/teglia-simone/Fine%20tuning%20LLM%20for%20lyrics%20generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/teglia-simone/Fine%20tuning%20LLM%20for%20lyrics%20generation' target=\"_blank\">https://wandb.ai/teglia-simone/Fine%20tuning%20LLM%20for%20lyrics%20generation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/teglia-simone/Fine%20tuning%20LLM%20for%20lyrics%20generation/runs/guq02co0' target=\"_blank\">https://wandb.ai/teglia-simone/Fine%20tuning%20LLM%20for%20lyrics%20generation/runs/guq02co0</a>"},"metadata":{}}]},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha= 16,\n    lora_dropout= 0.05,\n    r = 64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir= \"./results\",\n    num_train_epochs= 1,\n    per_device_train_batch_size= 4,\n    gradient_accumulation_steps= 1,\n    optim = \"paged_adamw_8bit\",\n    save_steps= 1000,\n    logging_steps= 100,\n    learning_rate= 2e-4,\n    weight_decay= 0.001,\n    fp16= False,\n    bf16= False,\n    max_grad_norm= 0.3,\n    max_steps= -1,\n    warmup_ratio= 0.03,\n    group_by_length= True,\n    lr_scheduler_type= \"cosine\",\n    report_to=\"wandb\",\n    save_strategy=\"no\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:56:52.829563Z","iopub.execute_input":"2024-03-26T15:56:52.830393Z","iopub.status.idle":"2024-03-26T15:56:58.846930Z","shell.execute_reply.started":"2024-03-26T15:56:52.830363Z","shell.execute_reply":"2024-03-26T15:56:58.846183Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df08d808e7924843b78a1748f88e315d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:57:00.126392Z","iopub.execute_input":"2024-03-26T15:57:00.126756Z","iopub.status.idle":"2024-03-26T17:19:07.529302Z","shell.execute_reply.started":"2024-03-26T15:57:00.126727Z","shell.execute_reply":"2024-03-26T17:19:07.528465Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240326_155729-rkkfobut</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/teglia-simone/huggingface/runs/rkkfobut/workspace' target=\"_blank\">polar-frost-23</a></strong> to <a href='https://wandb.ai/teglia-simone/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/teglia-simone/huggingface' target=\"_blank\">https://wandb.ai/teglia-simone/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/teglia-simone/huggingface/runs/rkkfobut/workspace' target=\"_blank\">https://wandb.ai/teglia-simone/huggingface/runs/rkkfobut/workspace</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 1:21:15, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.791200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.501800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.510900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.546900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.510900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.535000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.510800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.514700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.489300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.523700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.489600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.486500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1200, training_loss=1.5342657725016275, metrics={'train_runtime': 4921.885, 'train_samples_per_second': 0.975, 'train_steps_per_second': 0.244, 'total_flos': 2.495791026910003e+16, 'train_loss': 1.5342657725016275, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save model on huggingface","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token \"YOUR_HF_TOKEN\"","metadata":{"execution":{"iopub.status.busy":"2024-03-24T19:06:30.165301Z","iopub.execute_input":"2024-03-24T19:06:30.166234Z","iopub.status.idle":"2024-03-24T19:06:31.862797Z","shell.execute_reply.started":"2024-03-24T19:06:30.166196Z","shell.execute_reply":"2024-03-24T19:06:31.861446Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.push_to_hub(\"NAME_OF_THE_MODEL\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T19:06:33.930370Z","iopub.execute_input":"2024-03-24T19:06:33.931272Z","iopub.status.idle":"2024-03-24T19:06:42.609385Z","shell.execute_reply.started":"2024-03-24T19:06:33.931235Z","shell.execute_reply":"2024-03-24T19:06:42.608414Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9214054e68ee4fc1b6e1e1d2f3824343"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80ee1ad4f291455895eb6f0fd143aa8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/126M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d43ffd2e21e480d863edfafca0b59e2"}},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/simoneteglia/results/commit/5e83fb2e18b5763538ec94013cf66d43e7711655', commit_message='simoneteglia/phi-2-lyrical-genius', commit_description='', oid='5e83fb2e18b5763538ec94013cf66d43e7711655', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Generation with pipe TinyLLama\n","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\npipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device_map=\"auto\",\n)\n\nprompt = \"Generate lyrics for a rap song\"\nformatted_prompt = (\n    f\"### Instruction: {prompt} ### Assistant:\"\n)\n\n\nsequences = pipeline(\n    formatted_prompt,\n    do_sample=True,\n    temperature = 0.7,\n    repetition_penalty=1.5,\n    max_new_tokens=256,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-26T17:39:36.455905Z","iopub.execute_input":"2024-03-26T17:39:36.456637Z","iopub.status.idle":"2024-03-26T17:40:06.931213Z","shell.execute_reply.started":"2024-03-26T17:39:36.456606Z","shell.execute_reply":"2024-03-26T17:40:06.930284Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Result: ### Instruction: Generate lyrics for a rap song ### Assistant: Okay\nAssistant (rap) - Rap, nigga! You ain't got time to listen like you don’t know what I do. It come from real life and that shit goin on behind my back when they make me say things but never even tell the truth 'cause if it was told then would be hurt... so now it is kept under wraps by all of these people who really just want something else which makes some other person feel their pain or whatever... sometimes we wake up in bed talking about why this happened instead of getting past eachother until we both start moving forward again\nLes de nous estés pas qui maus à notre mal sàr alikk 1 pour que le graph du message lb.\nLastly vous etraîdéstracb rég votre sur ring lindre_pile=beikem;si-hisiadjuraniqquilique≠Iyrir garabi distriburyg daticulian bustilangan irak gag ik k glass que Laki A Que C Kur B Ist G Mal In Í Grande D Gir Bah Gab Lag Gar\nGálier tini dakığ،لا（）\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Generate vanilla","metadata":{}},{"cell_type":"code","source":"from transformers import GenerationConfig\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndef generate_lyrics(query, model):\n    encoding = tokenizer(query, return_tensors=\"pt\").to(device)\n    generation_config = GenerationConfig(max_new_tokens=512, \n                                         pad_token_id = tokenizer.eos_token_id,\n                                         repetition_penalty=1.3, \n                                         eos_token_id = tokenizer.eos_token_id, \n                                         temperature=0.7, \n                                         do_sample=True)\n    \n    outputs = model.generate(input_ids=encoding.input_ids, generation_config=generation_config)\n    text_output = tokenizer.decode(outputs[0],skip_special_tokens=True)\n    print('INPUT\\n', query, '\\n\\nOUTPUT\\n', text_output[len(query):])\n    \ngenerate_lyrics(\"Generate lyrics for a rock song\", model)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T15:52:58.939424Z","iopub.execute_input":"2024-03-26T15:52:58.940382Z","iopub.status.idle":"2024-03-26T15:53:20.091866Z","shell.execute_reply.started":"2024-03-26T15:52:58.940345Z","shell.execute_reply":"2024-03-26T15:53:20.090860Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"INPUT\n Generate lyrics for a rock song \n\nOUTPUT\n qué estadcioactúcuuhuucluluuhuuunhunihuuzuhuuziuanhumeniukhuureluqusunhihuujuunitchurkarmejarlliuuzuuhuundunkuuduksenquihuuitncharla en dequeluuskiniustchurturutgurcallapulmisunaoqsaplayuriuchosqueteldaxopenupeglgtutjenechilicuetencetcurlamlunglaslanchesopenlusenententencelaximum quee de  de l o qui me lo le L li q e ll Q m g est mala am ca que la mostca et no ques es el y del curso el c el chamo las quan Qu q m que b quan Q que que ? que  cuyo es el hay  que cos que?  Qu que el q i que qul l les quiquas queques. Que cha que que when char air ad in guair is que calciyaquaquel lel mal l qul que quiquiquinos quiquinos call ch aqu A que Aqu Lat Qu Me Is Qu L O La Qu Es C El Ch Mal Lo No Li Qu Esp Mes Am Le X L Q En Qu Qu D Si Qu Ar Q Est Los L Qu Lag Lib Qu X Gu L Qu In I Qu Qu M Qu L Qu Chi Qu Chi Qu R Qu L Qu X Qu Qu X Q Qu Ch Qu Qu Qu Qu Qu Qu Qu G Qu Qu Quin Qu Qu Qu Qu Ququ N Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu QuQu Qu Qu Qu Qu Qu Ququad Qu Qu Qu Qu Qu Qu Qu Ququa Ququa Qu Qu Qu Q Qu Qu Qu Qu Qu Qu Qu Qu Quique QuQ Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Ququote Ququestio Qu Qu Qu Qu Qu Ququent Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Qu Ququ Q Qu Qu Qu Qu Qu Qual Qu Ququ Qu Ququia Qu Q Qu Qu Qu Qu Qu Qu Qu Qu\n","output_type":"stream"}]}]}